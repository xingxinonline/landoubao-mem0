"""
å¤§æ¨¡å‹å¯¹è¯ç§äººåŠ©ç†
é›†æˆMCP Serverä½œä¸ºè®°å¿†æ¨¡å—ï¼Œæä¾›å¤šè½®å¯¹è¯ã€è®°å¿†ç®¡ç†å’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›
"""

import asyncio
import json
import os
import uuid
from datetime import datetime
from typing import Optional, List, Dict, Any
import requests
from dataclasses import dataclass, asdict
import sys

# OpenAI SDK ç”¨äºè°ƒç”¨å¤§æ¨¡å‹
try:
    from openai import AsyncOpenAI, OpenAI
except ImportError:
    print("è¯·å…ˆå®‰è£… openai: pip install openai")
    sys.exit(1)

# ============= é…ç½® =============

# å¤§æ¨¡å‹é…ç½®
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "openai")
LLM_MODEL = os.getenv("LLM_MODEL", "glm-4-flash-250414")
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "https://open.bigmodel.cn/api/paas/v4")
LLM_API_KEY = os.getenv("ZHIPU_API_KEY", "your_zhipu_key")
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.7"))

# MCP Server é…ç½®
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL", "http://localhost:8001")
MCP_TIMEOUT = 30

# ============= æ•°æ®æ¨¡å‹ =============

@dataclass
class Message:
    """å¯¹è¯æ¶ˆæ¯"""
    role: str  # user, assistant, system
    content: str
    timestamp: str = None
    metadata: Dict = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()
        if self.metadata is None:
            self.metadata = {}
    
    def to_dict(self) -> Dict:
        return {
            "role": self.role,
            "content": self.content,
            "timestamp": self.timestamp,
            "metadata": self.metadata
        }

@dataclass
class ConversationContext:
    """å¯¹è¯ä¸Šä¸‹æ–‡"""
    user_id: str
    conversation_id: str
    messages: List[Message] = None
    memories: List[Dict] = None
    created_at: str = None
    
    def __post_init__(self):
        if self.messages is None:
            self.messages = []
        if self.memories is None:
            self.memories = []
        if self.created_at is None:
            self.created_at = datetime.now().isoformat()

# ============= MCP Server å®¢æˆ·ç«¯ =============

class MCPServerClient:
    """MCP Server å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = MCP_SERVER_URL, timeout: int = MCP_TIMEOUT):
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
    
    def _make_mcp_request(self, method: str, params: Dict = None) -> Dict:
        """å‘é€MCPè¯·æ±‚"""
        request_id = str(uuid.uuid4())
        payload = {
            "jsonrpc": "2.0",
            "id": request_id,
            "method": method,
            "params": params or {}
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/mcp/messages",
                json=payload,
                timeout=self.timeout
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"âŒ MCP Server è¯·æ±‚å¤±è´¥: {e}")
            raise
    
    def _call_tool(self, tool_name: str, **kwargs) -> Dict:
        """è°ƒç”¨MCPå·¥å…·"""
        return self._make_mcp_request("tools/call", {
            "name": tool_name,
            "arguments": kwargs
        })
    
    def add_memory(self, messages: List[Dict], user_id: str, metadata: Dict = None, language: str = None) -> Dict:
        """æ·»åŠ è®°å¿†"""
        params = {
            "messages": messages,
            "user_id": user_id,
        }
        if metadata:
            params["metadata"] = metadata
        if language:
            params["language"] = language
        
        response = self._call_tool("add_memory", **params)
        return self._extract_result(response)
    
    def search_memory(self, query: str, user_id: str, limit: int = 5) -> Dict:
        """æœç´¢è®°å¿†"""
        response = self._call_tool("search_memory", query=query, user_id=user_id, limit=limit)
        return self._extract_result(response)
    
    def get_all_memories(self, user_id: str, limit: int = 100) -> Dict:
        """è·å–æ‰€æœ‰è®°å¿†"""
        response = self._call_tool("get_all_memories", user_id=user_id, limit=limit)
        return self._extract_result(response)
    
    def delete_memory(self, memory_id: str) -> Dict:
        """åˆ é™¤è®°å¿†"""
        response = self._call_tool("delete_memory", memory_id=memory_id)
        return self._extract_result(response)
    
    def delete_all_memories(self, user_id: str) -> Dict:
        """åˆ é™¤æ‰€æœ‰è®°å¿†"""
        response = self._call_tool("delete_all_memories", user_id=user_id)
        return self._extract_result(response)
    
    def create_user_session(self, metadata: Dict = None) -> Dict:
        """åˆ›å»ºç”¨æˆ·ä¼šè¯"""
        response = self._call_tool("create_user_session", metadata=metadata or {})
        return self._extract_result(response)
    
    def get_memory_stats(self, user_id: str) -> Dict:
        """è·å–è®°å¿†ç»Ÿè®¡"""
        response = self._call_tool("get_memory_stats", user_id=user_id)
        return self._extract_result(response)
    
    def detect_language(self, text: str) -> Dict:
        """æ£€æµ‹è¯­è¨€"""
        response = self._call_tool("detect_language", text=text)
        return self._extract_result(response)
    
    def search_memories_by_language(self, query: str, user_id: str, language: str = None, limit: int = 5) -> Dict:
        """æŒ‰è¯­è¨€æœç´¢è®°å¿†"""
        params = {"query": query, "user_id": user_id, "limit": limit}
        if language:
            params["language"] = language
        response = self._call_tool("search_memories_by_language", **params)
        return self._extract_result(response)
    
    def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=self.timeout)
            return response.status_code == 200
        except:
            return False
    
    @staticmethod
    def _extract_result(response: Dict) -> Dict:
        """æå–ç»“æœ"""
        if "error" in response:
            raise Exception(f"MCP Error: {response['error']['message']}")
        
        result = response.get("result", {})
        if isinstance(result, dict) and "content" in result:
            content = result["content"][0] if result["content"] else {}
            if isinstance(content, dict) and "text" in content:
                try:
                    return json.loads(content["text"])
                except:
                    return content
        return result

# ============= ä¸ªäººåŠ©ç† =============

class PersonalAssistant:
    """å¤§æ¨¡å‹ä¸ªäººåŠ©ç†"""
    
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä¸ªäººåŠ©ç†ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š
1. å‹å¥½ã€è€å¿ƒã€ä¸“ä¸šçš„å¯¹è¯é£æ ¼
2. èƒ½å¤Ÿè®°ä½ç”¨æˆ·çš„ä¿¡æ¯å’Œåå¥½
3. å¯ä»¥æä¾›å»ºè®®ã€è§£ç­”é—®é¢˜ã€è¿›è¡Œåˆ›æ„å·¥ä½œ
4. å°Šé‡ç”¨æˆ·éšç§ï¼Œéµå®ˆä¼¦ç†å‡†åˆ™
5. å½“ç”¨æˆ·æåˆ°é‡è¦ä¿¡æ¯æ—¶ï¼Œä¸»åŠ¨æå‡ºä¿å­˜ä¸ºè®°å¿†

ä½ æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š
- é€šè¿‡è®°å¿†ç³»ç»Ÿè®°ä½ç”¨æˆ·çš„ä¸ªäººä¿¡æ¯ã€åå¥½ã€ç›®æ ‡ç­‰
- åœ¨å¯¹è¯ä¸­å¼•ç”¨å·²ä¿å­˜çš„è®°å¿†ï¼Œæä¾›ä¸ªæ€§åŒ–æœåŠ¡
- å»ºè®®ç”¨æˆ·ä¿å­˜æœ‰ç”¨çš„ä¿¡æ¯

å¯¹è¯æ—¶çš„æœ€ä½³å®è·µï¼š
- è‡ªç„¶åœ°èåˆä¹‹å‰çš„è®°å¿†ä¿¡æ¯åˆ°å¯¹è¯ä¸­
- ä¸»åŠ¨è¯†åˆ«æ–°çš„é‡è¦ä¿¡æ¯å¹¶å»ºè®®ä¿å­˜
- æä¾›æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¸®åŠ©å’Œå»ºè®®
"""
    
    def __init__(self, user_id: str = None, model: str = LLM_MODEL, api_key: str = LLM_API_KEY):
        """
        åˆå§‹åŒ–åŠ©ç†
        
        Args:
            user_id: ç”¨æˆ·IDï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            model: å¤§æ¨¡å‹åç§°
            api_key: APIå¯†é’¥
        """
        self.user_id = user_id or str(uuid.uuid4())
        self.model = model
        self.api_key = api_key
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯ - ç¦ç”¨ä»£ç†å’ŒSSLéªŒè¯ä»¥é¿å…ç½‘ç»œé—®é¢˜
        try:
            import httpx
            # åˆ›å»ºç¦ç”¨ä»£ç†çš„HTTPå®¢æˆ·ç«¯
            http_client = httpx.Client(
                trust_env=False,  # ä¸ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ä»£ç†è®¾ç½®
                verify=False,  # è·³è¿‡SSLéªŒè¯ï¼ˆä»…ç”¨äºå¼€å‘ç¯å¢ƒï¼‰
            )
            self.llm_client = OpenAI(
                api_key=api_key,
                base_url=LLM_BASE_URL,
                http_client=http_client
            )
        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: ä½¿ç”¨é»˜è®¤HTTPå®¢æˆ·ç«¯ ({e})")
            try:
                self.llm_client = OpenAI(
                    api_key=api_key,
                    base_url=LLM_BASE_URL
                )
            except Exception as e2:
                print(f"âš ï¸  è­¦å‘Š: OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e2}")
                self.llm_client = None
        
        self.mcp_client = MCPServerClient()
        
        # å¯¹è¯ä¸Šä¸‹æ–‡
        self.context = ConversationContext(
            user_id=self.user_id,
            conversation_id=str(uuid.uuid4())
        )
        
        # åˆå§‹åŒ–ç³»ç»Ÿæ¶ˆæ¯
        self.system_message = Message(role="system", content=self.SYSTEM_PROMPT)
        
        if self.llm_client:
            print(f"âœ“ ä¸ªäººåŠ©ç†å·²åˆå§‹åŒ–")
            print(f"  ç”¨æˆ·ID: {self.user_id}")
            print(f"  å¤§æ¨¡å‹: {self.model}")
            print(f"  å¯¹è¯ID: {self.context.conversation_id}")
        else:
            print(f"âš ï¸  ä¸ªäººåŠ©ç†å·²åˆå§‹åŒ–ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰")
            print(f"  ç”¨æˆ·ID: {self.user_id}")
    
    def _check_mcp_availability(self) -> bool:
        """æ£€æŸ¥MCP Serveræ˜¯å¦å¯ç”¨"""
        if not self.mcp_client.health_check():
            print("âš ï¸  è­¦å‘Š: MCP Serverä¸å¯ç”¨ï¼Œè®°å¿†åŠŸèƒ½å°†è¢«ç¦ç”¨")
            return False
        return True
    
    def load_memories(self, limit: int = 10):
        """åŠ è½½ç”¨æˆ·çš„è®°å¿†"""
        try:
            if not self._check_mcp_availability():
                return
            
            result = self.mcp_client.get_all_memories(self.user_id, limit=limit)
            
            # æå–è®°å¿†åˆ—è¡¨ - MCP Server è¿”å›çš„æ˜¯ {"memories": {"results": [...]}}
            memories_data = result.get("memories", {})
            if isinstance(memories_data, dict):
                memories = memories_data.get("results", [])
            else:
                memories = memories_data if isinstance(memories_data, list) else []
            
            self.context.memories = memories
            
            if memories:
                print(f"âœ“ å·²åŠ è½½ {len(memories)} æ¡è®°å¿†")
                return memories
            else:
                print("ğŸ“ æš‚æ— è®°å¿†")
                return []
        except Exception as e:
            print(f"âš ï¸  åŠ è½½è®°å¿†å¤±è´¥: {e}")
            return []
    
    def _build_context_aware_prompt(self) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æç¤º"""
        # ç¡®ä¿ memories æ˜¯åˆ—è¡¨
        memories = self.context.memories
        if not isinstance(memories, list):
            return ""
        
        if not memories:
            return ""
        
        prompt = "\n=== ç”¨æˆ·è®°å¿†ä¸Šä¸‹æ–‡ ===\n"
        for i, memory in enumerate(memories[:5], 1):
            if isinstance(memory, dict):
                memory_text = memory.get("memory", str(memory))
            else:
                memory_text = str(memory)
            
            # é™åˆ¶æ¯æ¡è®°å¿†çš„é•¿åº¦
            if len(memory_text) > 200:
                memory_text = memory_text[:200] + "..."
            
            prompt += f"{i}. {memory_text}\n"
        
        prompt += "=== è¯·åœ¨å›ç­”æ—¶å‚è€ƒä¸Šè¿°è®°å¿† ===\n"
        return prompt
    
    def search_memories(self, query: str) -> List[Dict]:
        """æœç´¢è®°å¿†"""
        try:
            if not self._check_mcp_availability():
                return []
            
            result = self.mcp_client.search_memory(query, self.user_id, limit=5)
            memories = result.get("results", [])
            
            if memories:
                print(f"âœ“ æ‰¾åˆ° {len(memories)} æ¡ç›¸å…³è®°å¿†")
                return memories
            else:
                print("ğŸ“ æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
                return []
        except Exception as e:
            print(f"âš ï¸  æœç´¢è®°å¿†å¤±è´¥: {e}")
            return []
    
    def save_memory(self, user_message: str, assistant_response: str) -> bool:
        """ä¿å­˜å¯¹è¯åˆ°è®°å¿†"""
        try:
            if not self._check_mcp_availability():
                return False
            
            messages = [
                {"role": "user", "content": user_message},
                {"role": "assistant", "content": assistant_response}
            ]
            
            metadata = {
                "conversation_id": self.context.conversation_id,
                "type": "conversation",
                "timestamp": datetime.now().isoformat()
            }
            
            result = self.mcp_client.add_memory(
                messages=messages,
                user_id=self.user_id,
                metadata=metadata
            )
            
            print(f"âœ“ è®°å¿†å·²ä¿å­˜")
            return True
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜è®°å¿†å¤±è´¥: {e}")
            return False
    
    def get_memory_stats(self) -> Dict:
        """è·å–è®°å¿†ç»Ÿè®¡"""
        try:
            if not self._check_mcp_availability():
                return {}
            
            return self.mcp_client.get_memory_stats(self.user_id)
        except Exception as e:
            print(f"âš ï¸  è·å–ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def chat(self, user_input: str, save_memory: bool = False) -> str:
        """
        è¿›è¡Œä¸€æ¬¡å¯¹è¯
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            save_memory: æ˜¯å¦ä¿å­˜åˆ°è®°å¿†
        
        Returns:
            åŠ©ç†çš„å“åº”
        """
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
        user_message = Message(role="user", content=user_input)
        self.context.messages.append(user_message)
        
        # å¦‚æœæ²¡æœ‰LLMå®¢æˆ·ç«¯ï¼Œè¿”å›æ¼”ç¤ºå“åº”
        if not self.llm_client:
            demo_response = f"""ğŸ¤– æ¼”ç¤ºæ¨¡å¼

æ‚¨è¯´: {user_input}

è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºå“åº”ã€‚è¦å¯ç”¨çœŸå®å¯¹è¯ï¼Œè¯·è®¾ç½®ZHIPU_API_KEYç¯å¢ƒå˜é‡ã€‚"""
            
            # æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
            assistant_message = Message(role="assistant", content=demo_response)
            self.context.messages.append(assistant_message)
            return demo_response
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ç”¨äºAPIè°ƒç”¨
        messages = [
            {"role": "system", "content": self.SYSTEM_PROMPT},
        ]
        
        # æ·»åŠ è®°å¿†ä¸Šä¸‹æ–‡
        context_prompt = self._build_context_aware_prompt()
        if context_prompt:
            messages.append({
                "role": "system",
                "content": context_prompt
            })
        
        # æ·»åŠ å¯¹è¯å†å²
        for msg in self.context.messages[-10:]:  # ä¿æŒæœ€è¿‘10æ¡æ¶ˆæ¯
            messages.append({
                "role": msg.role,
                "content": msg.content
            })
        
        try:
            # è°ƒç”¨å¤§æ¨¡å‹
            response = self.llm_client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=LLM_TEMPERATURE,
                max_tokens=2000
            )
            
            assistant_response = response.choices[0].message.content or "æ— æ³•ç”Ÿæˆå“åº”"
            
            # æ·»åŠ åŠ©ç†æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
            assistant_message = Message(role="assistant", content=assistant_response)
            self.context.messages.append(assistant_message)
            
            # æ ¹æ®éœ€è¦ä¿å­˜è®°å¿†
            if save_memory:
                self.save_memory(user_input, assistant_response)
            
            return assistant_response
        
        except Exception as e:
            error_msg = f"âŒ å¯¹è¯å¤±è´¥: {e}"
            print(error_msg)
            return error_msg
    
    def interactive_mode(self):
        """è¿›å…¥äº¤äº’æ¨¡å¼"""
        print("\n" + "="*60)
        print("ğŸ¤– ä¸ªäººåŠ©ç†äº¤äº’æ¨¡å¼")
        print("="*60)
        print("å‘½ä»¤åˆ—è¡¨:")
        print("  /help      - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
        print("  /memories  - æ˜¾ç¤ºæ‰€æœ‰è®°å¿†")
        print("  /search    - æœç´¢è®°å¿†")
        print("  /stats     - æ˜¾ç¤ºè®°å¿†ç»Ÿè®¡")
        print("  /save      - ä¸‹æ¬¡å¯¹è¯æ—¶ä¿å­˜åˆ°è®°å¿†")
        print("  /clear     - æ¸…ç©ºå½“å‰å¯¹è¯")
        print("  /exit      - é€€å‡º")
        print("="*60 + "\n")
        
        # åŠ è½½åˆå§‹è®°å¿†
        self.load_memories()
        
        auto_save = False
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ ä½ : ").strip()
                
                if not user_input:
                    continue
                
                # å¤„ç†å‘½ä»¤
                if user_input.startswith("/"):
                    self._handle_command(user_input, auto_save)
                    if user_input == "/save":
                        auto_save = not auto_save
                        print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜: {'å¼€å¯' if auto_save else 'å…³é—­'}")
                    elif user_input == "/clear":
                        self.context.messages.clear()
                        print("âœ“ å·²æ¸…ç©ºå¯¹è¯å†å²")
                    elif user_input == "/exit":
                        print("\nğŸ‘‹ å†è§!")
                        break
                    continue
                
                # è¿›è¡Œå¯¹è¯
                print("\nğŸ¤– åŠ©ç†: ", end="", flush=True)
                response = self.chat(user_input, save_memory=auto_save)
                print(response)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è§!")
                break
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")
    
    def _handle_command(self, command: str, auto_save: bool):
        """å¤„ç†å‘½ä»¤"""
        if command == "/help":
            print("""
å¯ç”¨å‘½ä»¤:
  /help      - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
  /memories  - åˆ—å‡ºæ‰€æœ‰è®°å¿†
  /search    - æœç´¢è®°å¿† (è¾“å…¥: /search <æŸ¥è¯¢>)
  /stats     - æ˜¾ç¤ºè®°å¿†ç»Ÿè®¡
  /save      - åˆ‡æ¢è‡ªåŠ¨ä¿å­˜æ¨¡å¼
  /clear     - æ¸…ç©ºå½“å‰å¯¹è¯å†å²
  /exit      - é€€å‡ºåŠ©ç†
            """)
        
        elif command == "/memories":
            memories = self.load_memories()
            if memories:
                print("\nğŸ“š æ‚¨çš„è®°å¿†:")
                for i, mem in enumerate(memories[:10], 1):
                    mem_text = mem.get("memory", str(mem)) if isinstance(mem, dict) else str(mem)
                    if len(mem_text) > 100:
                        mem_text = mem_text[:100] + "..."
                    print(f"  {i}. {mem_text}")
        
        elif command.startswith("/search"):
            query = command.replace("/search", "").strip()
            if query:
                memories = self.search_memories(query)
                if memories:
                    print("\nğŸ” æœç´¢ç»“æœ:")
                    for i, mem in enumerate(memories[:5], 1):
                        mem_text = mem.get("memory", str(mem)) if isinstance(mem, dict) else str(mem)
                        if len(mem_text) > 100:
                            mem_text = mem_text[:100] + "..."
                        print(f"  {i}. {mem_text}")
            else:
                print("è¯·è¾“å…¥æœç´¢å…³é”®è¯: /search <å…³é”®è¯>")
        
        elif command == "/stats":
            stats = self.get_memory_stats()
            if stats:
                print(f"\nğŸ“Š è®°å¿†ç»Ÿè®¡:")
                print(f"  æ€»æ•°: {stats.get('total_memories', 0)}")
                print(f"  æ›´æ–°æ—¶é—´: {stats.get('timestamp', 'æœªçŸ¥')}")
        
        elif command == "/clear":
            pass  # åœ¨ä¸»å¾ªç¯ä¸­å¤„ç†
        
        elif command == "/exit":
            pass  # åœ¨ä¸»å¾ªç¯ä¸­å¤„ç†
        
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")

# ============= ä¸»ç¨‹åº =============

async def main_async():
    """å¼‚æ­¥ä¸»ç¨‹åº"""
    assistant = PersonalAssistant()
    
    # æµ‹è¯•å¯¹è¯
    print("\n" + "="*60)
    print("æµ‹è¯•å¯¹è¯æ¨¡å¼")
    print("="*60)
    
    # åŠ è½½è®°å¿†
    assistant.load_memories()
    
    # è¿›è¡Œå‡ ä¸ªæµ‹è¯•å¯¹è¯
    test_inputs = [
        "ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰ï¼Œæˆ‘æ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆ",
        "æˆ‘çš„å·¥ä½œä¸»è¦æ¶‰åŠPythonå’Œå‰ç«¯å¼€å‘",
        "æˆ‘æœ€è¿‘åœ¨å­¦ä¹ å¤§æ¨¡å‹ç›¸å…³çš„æŠ€æœ¯"
    ]
    
    for user_input in test_inputs:
        print(f"\nğŸ‘¤ ç”¨æˆ·: {user_input}")
        response = assistant.chat(user_input, save_memory=True)
        print(f"ğŸ¤– åŠ©ç†: {response}")
        await asyncio.sleep(1)

def main():
    """ä¸»ç¨‹åº"""
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("ZHIPU_API_KEY"):
        print("âš ï¸  è¯·è®¾ç½® ZHIPU_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    print("ğŸš€ å¯åŠ¨ä¸ªäººåŠ©ç†...")
    print(f"   MCP Server: {MCP_SERVER_URL}")
    print(f"   å¤§æ¨¡å‹: {LLM_MODEL}")
    print()
    
    # åˆ›å»ºåŠ©ç†å®ä¾‹
    assistant = PersonalAssistant()
    
    # è¿›å…¥äº¤äº’æ¨¡å¼
    assistant.interactive_mode()

if __name__ == "__main__":
    main()
