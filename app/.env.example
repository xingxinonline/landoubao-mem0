ZHIPU_API_KEY=your_zhipu_api_key_here
MODELARK_API_KEY=your_modelark_api_key_here

# ========== Qdrant Vector Store Configuration ==========
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION_NAME=mem0

# ========== Concurrency Settings ==========
MAX_CONCURRENT_REQUESTS=20
REQUEST_TIMEOUT=60
THREAD_POOL_SIZE=10

# ========== Production Settings ==========
# Uvicorn workers (for production, set to CPU cores)
UVICORN_WORKERS=1
# Keep alive timeout
UVICORN_KEEP_ALIVE=75
# Limit max requests per worker (memory leak protection)
UVICORN_MAX_REQUESTS=10000
UVICORN_MAX_REQUESTS_JITTER=1000

# ========== LLM Configuration (Zhipu AI) ==========
# LLM provider (default: openai for OpenAI-compatible API)
LLM_PROVIDER=openai
# Zhipu AI model
LLM_MODEL=glm-4-flash-250414
# Zhipu AI API endpoint
LLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4
# LLM generation temperature (0.0-2.0, higher = more creative)
LLM_TEMPERATURE=0.1
# Maximum tokens to generate
LLM_MAX_TOKENS=2000

# ========== Embedding Configuration (ModelArk) ==========
# Embedding provider (default: openai for OpenAI-compatible API)
EMBEDDING_PROVIDER=openai
# Embedding model options:
#   - bge-m3 (1024 dims, multilingual, excellent for 100+ languages) 
#   - Qwen3-Embedding-0.6B (1024 dims, low resource) - RECOMMENDED for production
#   - Qwen3-Embedding-8B (4096 dims, high quality)
EMBEDDING_MODEL=Qwen3-Embedding-0.6B
# Embedding output dimensions (must match model: 1024 or 4096)
EMBEDDING_DIMS=1024
# ModelArk API endpoint
EMBEDDING_BASE_URL=https://ai.gitee.com/v1
